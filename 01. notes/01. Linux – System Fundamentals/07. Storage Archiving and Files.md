# Storage, Archiving & Files

Data lives on storage. Logs accumulate, databases grow, backups pile up, applications deploy. Understanding how Linux manages storage — disks, partitions, filesystems, and the tools to manipulate files — is essential for keeping systems healthy and data safe.

This isn't just about running out of disk space (though you'll handle that too). It's about understanding how data is organized from raw block devices up through mounted filesystems. It's about compressing logs, creating archives, transferring backups, and recovering when things go wrong.

By the end of this file, you'll manage disks and partitions, work with different filesystems, monitor space usage, and handle archiving and compression like second nature.

---

## Table of Contents

1. [Storage Concepts](#1-storage-concepts)
2. [Block Devices and Partitions](#2-block-devices-and-partitions)
3. [Filesystems](#3-filesystems)
4. [Mounting and Unmounting](#4-mounting-and-unmounting)
5. [Disk Usage and Monitoring](#5-disk-usage-and-monitoring)
6. [File Operations at Scale](#6-file-operations-at-scale)
7. [Compression](#7-compression)
8. [Archiving with tar](#8-archiving-with-tar)
9. [Transferring Files](#9-transferring-files)
10. [Where We Go From Here](#10-where-we-go-from-here)

---

<details>
<summary><strong>1. Storage Concepts</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Before diving into commands, understand the storage hierarchy.

### The layers

**Physical storage** — The actual hardware: SSDs, HDDs, NVMe drives, virtual disks in cloud environments.

**Block devices** — How the kernel sees storage. Each device gets a name like `/dev/sda` or `/dev/nvme0n1`. You interact with storage through these device files.

**Partitions** — Divisions of a block device. A single disk might have `/dev/sda1`, `/dev/sda2`, etc. Each partition can be used independently.

**Filesystems** — The structure that organizes data within a partition. ext4, XFS, and others define how files and directories are stored and retrieved.

**Mount points** — Where filesystems attach to the directory tree. A filesystem on `/dev/sda1` might be mounted at `/home`, making its contents accessible there.

### Device naming

**Traditional SATA/SAS:**
- `/dev/sda` — First disk
- `/dev/sdb` — Second disk
- `/dev/sda1` — First partition on first disk

**NVMe drives:**
- `/dev/nvme0n1` — First NVMe drive
- `/dev/nvme0n1p1` — First partition

**Virtual disks (cloud/VM):**
- `/dev/vda`, `/dev/xvda` — Varies by hypervisor

### Everything is a file (again)

Block devices are files in `/dev`. When you read from `/dev/sda`, you're reading raw bytes from the disk. This uniformity means standard tools work on storage — you can `dd` an entire disk, `cat` a partition, or redirect output directly to devices (carefully).

</div>
</details>

---

<details>
<summary><strong>2. Block Devices and Partitions</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Managing storage starts with seeing what you have and how it's divided.

### Listing block devices

```bash
lsblk
```

Output:
```
NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
sda           8:0    0   100G  0 disk 
├─sda1        8:1    0     1G  0 part /boot
├─sda2        8:2    0    50G  0 part /
└─sda3        8:3    0    49G  0 part /home
sdb           8:16   0   500G  0 disk 
└─sdb1        8:17   0   500G  0 part /data
```

Shows disks, partitions, sizes, and mount points at a glance.

### Detailed disk information

```bash
sudo fdisk -l
```

Lists all disks with partition tables, sizes, types, and sector information.

```bash
sudo blkid
```

Shows UUIDs and filesystem types — useful for persistent mounting.

### Partitioning tools

**fdisk** — Traditional MBR partitioning (also handles GPT now):

```bash
sudo fdisk /dev/sdb
```

Interactive menu: `n` for new partition, `d` for delete, `p` to print, `w` to write changes.

**parted** — Handles both MBR and GPT, supports larger disks:

```bash
sudo parted /dev/sdb
(parted) print                       # Show partitions
(parted) mklabel gpt                 # Create GPT partition table
(parted) mkpart primary ext4 0% 100% # Create partition using full disk
```

**gdisk** — GPT-specific tool, similar interface to fdisk:

```bash
sudo gdisk /dev/sdb
```

### MBR vs GPT

**MBR** (Master Boot Record) — Legacy. Max 4 primary partitions, 2TB disk limit. Still common on older systems.

**GPT** (GUID Partition Table) — Modern. 128+ partitions, supports huge disks. Required for UEFI boot. Use this for new systems.

### Creating a partition workflow

```bash
# 1. Create partition
sudo fdisk /dev/sdb
# n -> new, accept defaults or specify size, w -> write

# 2. Verify
lsblk

# 3. Create filesystem (next section)
sudo mkfs.ext4 /dev/sdb1

# 4. Mount (later section)
sudo mount /dev/sdb1 /data
```

</div>
</details>

---

<details>
<summary><strong>3. Filesystems</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

A filesystem organizes raw storage into files and directories. Different filesystems have different features, performance characteristics, and use cases.

### Common Linux filesystems

| Filesystem | Use Case | Notes |
|------------|----------|-------|
| ext4 | General purpose | Default on most Linux distros, mature, reliable |
| XFS | Large files, high performance | Default on RHEL, great for databases |
| Btrfs | Snapshots, compression | Advanced features, good for complex setups |
| tmpfs | RAM-based temporary storage | Contents lost on reboot, very fast |
| swap | Virtual memory | Not a traditional filesystem |

### Creating filesystems

```bash
sudo mkfs.ext4 /dev/sdb1             # ext4
sudo mkfs.xfs /dev/sdb1              # XFS
sudo mkfs.btrfs /dev/sdb1            # Btrfs
```

**Warning:** This destroys all data on the partition.

### Filesystem labels and UUIDs

Labels make identification easier:

```bash
sudo e2label /dev/sdb1 "data"        # Set label (ext4)
sudo xfs_admin -L "data" /dev/sdb1   # Set label (XFS)
```

UUIDs are unique identifiers assigned at creation:

```bash
sudo blkid /dev/sdb1
```

Output: `/dev/sdb1: UUID="a1b2c3d4-..." TYPE="ext4" LABEL="data"`

### Checking and repairing

Filesystems can become inconsistent after crashes or hardware issues.

**ext4:**

```bash
sudo fsck.ext4 /dev/sdb1             # Check and repair
sudo fsck -y /dev/sdb1               # Auto-yes to repairs
```

**XFS:**

```bash
sudo xfs_repair /dev/sdb1
```

**Important:** Never run fsck on a mounted filesystem. Unmount first, or boot to recovery mode for root filesystem.

### Filesystem information

```bash
sudo tune2fs -l /dev/sdb1            # ext4 details
sudo xfs_info /dev/sdb1              # XFS details
df -T                                 # Show filesystem types for mounted systems
```

</div>
</details>

---

<details>
<summary><strong>4. Mounting and Unmounting</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

A filesystem isn't accessible until it's mounted — attached to a directory in the filesystem tree.

### Manual mounting

```bash
sudo mount /dev/sdb1 /data
```

Now `/data` contains whatever is on `/dev/sdb1`.

**With options:**

```bash
sudo mount -o ro /dev/sdb1 /data     # Read-only
sudo mount -o noexec /dev/sdb1 /data # Prevent execution
sudo mount -t ext4 /dev/sdb1 /data   # Specify type
```

### Unmounting

```bash
sudo umount /data
# or
sudo umount /dev/sdb1
```

If unmount fails ("target is busy"), something is using the filesystem:

```bash
lsof +D /data                        # Find what's using it
fuser -m /data                       # Show processes
sudo fuser -km /data                 # Kill processes (careful!)
```

### Persistent mounts with /etc/fstab

Manual mounts don't survive reboot. For persistence, edit `/etc/fstab`:

```
# <device>              <mount>   <type>  <options>        <dump> <pass>
UUID=a1b2c3d4-...       /data     ext4    defaults         0      2
/dev/sdb1               /backup   xfs     defaults,noatime 0      2
```

| Field | Purpose |
|-------|---------|
| device | UUID, label, or device path |
| mount | Mount point directory |
| type | Filesystem type |
| options | Mount options (defaults is common) |
| dump | Backup flag (usually 0) |
| pass | fsck order (0=skip, 1=root, 2=others) |

**Using UUIDs is recommended** — device names can change, UUIDs don't.

After editing fstab:

```bash
sudo mount -a                        # Mount everything in fstab
```

### Common mount options

| Option | Effect |
|--------|--------|
| defaults | rw, suid, dev, exec, auto, nouser, async |
| ro | Read-only |
| noexec | Prevent binary execution |
| nosuid | Ignore setuid bits |
| noatime | Don't update access times (performance) |
| nofail | Boot continues if mount fails |

For ChillSpot's data volume:

```
UUID=xxx  /srv/chillspot-streaming  ext4  defaults,noatime  0  2
```

### Temporary filesystems

tmpfs lives in RAM:

```bash
sudo mount -t tmpfs -o size=1G tmpfs /mnt/ramdisk
```

Useful for high-speed temporary data. Add to fstab:

```
tmpfs  /mnt/ramdisk  tmpfs  size=1G  0  0
```

</div>
</details>

---

<details>
<summary><strong>5. Disk Usage and Monitoring</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Running out of disk space crashes services, corrupts databases, and ruins your day. Monitoring usage is essential.

### df — Filesystem space

```bash
df -h                                # Human-readable sizes
```

Output:
```
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2        50G   32G   15G  69% /
/dev/sda3        49G   28G   19G  60% /home
/dev/sdb1       500G  350G  125G  74% /data
```

```bash
df -h /data                          # Specific mount point
df -i                                # Inode usage (file count limits)
```

### du — Directory space

```bash
du -sh /var/log                      # Total size of directory
du -sh /*                            # Size of each top-level directory
du -h --max-depth=1 /home            # One level deep
```

**Find largest directories:**

```bash
du -h /var | sort -rh | head -20
```

### Finding large files

```bash
find / -type f -size +100M -exec ls -lh {} \; 2>/dev/null
```

Or with more detail:

```bash
find / -type f -size +100M -printf "%s %p\n" 2>/dev/null | sort -rn | head -20
```

### ncdu — Interactive disk usage

If installed, `ncdu` provides a browsable interface:

```bash
ncdu /var
```

Navigate with arrow keys, delete files with `d`. Install with `apt install ncdu` or `dnf install ncdu`.

### Monitoring over time

**Set up alerts** when usage exceeds thresholds. A simple check:

```bash
#!/bin/bash
THRESHOLD=80
USAGE=$(df / | tail -1 | awk '{print $5}' | tr -d '%')
if [ "$USAGE" -gt "$THRESHOLD" ]; then
    echo "Disk usage critical: ${USAGE}%" | mail -s "Disk Alert" admin@chillspot.io
fi
```

Run via cron for regular checks.

### Cleaning up space

Common space consumers:

```bash
# Package cache
sudo apt clean                       # Debian/Ubuntu
sudo dnf clean all                   # RHEL

# Old logs
sudo journalctl --vacuum-time=7d     # Keep only 7 days of journal

# Old kernels (Ubuntu)
sudo apt autoremove --purge

# Find and review large files
find /var/log -name "*.log" -size +100M
```

</div>
</details>

---

<details>
<summary><strong>6. File Operations at Scale</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Beyond basic `cp` and `mv`, you need tools that handle large numbers of files efficiently.

### rsync — The power tool

`rsync` copies files efficiently, transferring only differences:

```bash
rsync -av /source/ /destination/
```

| Flag | Effect |
|------|--------|
| -a | Archive mode (preserves permissions, timestamps, symlinks) |
| -v | Verbose |
| -z | Compress during transfer |
| --delete | Remove files in destination not in source |
| --dry-run | Show what would happen without doing it |
| --progress | Show transfer progress |

**Local sync:**

```bash
rsync -av /srv/chillspot-streaming/ /backup/chillspot/
```

**Remote sync:**

```bash
rsync -avz /local/path/ user@remote:/remote/path/
rsync -avz user@remote:/remote/path/ /local/path/
```

**Mirror with deletion:**

```bash
rsync -av --delete /source/ /destination/
```

Destination becomes exact mirror — files not in source are removed.

**Always test with --dry-run first:**

```bash
rsync -av --delete --dry-run /source/ /destination/
```

### dd — Low-level copying

`dd` copies raw bytes — useful for disk images, backups, and special operations:

```bash
# Clone a disk
sudo dd if=/dev/sda of=/dev/sdb bs=64K status=progress

# Create disk image
sudo dd if=/dev/sda of=/backup/disk.img bs=64K status=progress

# Write image to disk
sudo dd if=disk.img of=/dev/sdb bs=64K status=progress
```

**Warning:** `dd` doesn't ask for confirmation. Wrong `of=` target destroys data instantly.

### Handling many files

When dealing with thousands of files, some commands choke. Use `find` with `-exec` or `xargs`:

```bash
# Delete old log files
find /var/log -name "*.log" -mtime +30 -exec rm {} \;

# More efficient with xargs
find /var/log -name "*.log" -mtime +30 | xargs rm

# Handle filenames with spaces
find /var/log -name "*.log" -mtime +30 -print0 | xargs -0 rm
```

### Parallel operations

For large transfers, parallelize:

```bash
# GNU Parallel
find /source -type f | parallel -j4 cp {} /destination/

# Or rsync multiple directories
rsync -av /source/dir1/ /dest/dir1/ &
rsync -av /source/dir2/ /dest/dir2/ &
wait
```

</div>
</details>

---

<details>
<summary><strong>7. Compression</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Compression reduces file sizes — essential for logs, backups, and transfers.

### Common compression tools

| Tool | Extension | Speed | Compression | Notes |
|------|-----------|-------|-------------|-------|
| gzip | .gz | Fast | Good | Most common |
| bzip2 | .bz2 | Slow | Better | Higher compression |
| xz | .xz | Slowest | Best | Maximum compression |
| zstd | .zst | Very fast | Very good | Modern, excellent balance |

### gzip — The standard

```bash
gzip file.log                        # Compress → file.log.gz (original deleted)
gzip -k file.log                     # Keep original
gzip -9 file.log                     # Maximum compression
gunzip file.log.gz                   # Decompress
zcat file.log.gz                     # View without decompressing
```

### bzip2 — Higher compression

```bash
bzip2 file.log                       # Compress → file.log.bz2
bunzip2 file.log.bz2                 # Decompress
bzcat file.log.bz2                   # View without decompressing
```

### xz — Maximum compression

```bash
xz file.log                          # Compress → file.log.xz
unxz file.log.xz                     # Decompress
xzcat file.log.xz                    # View without decompressing
```

### zstd — Modern choice

Fast compression with excellent ratios:

```bash
zstd file.log                        # Compress → file.log.zst
zstd -d file.log.zst                 # Decompress
zstdcat file.log.zst                 # View without decompressing
```

Install with `apt install zstd` or `dnf install zstd`.

### Comparing compression

For a 100MB log file:

| Tool | Size | Time |
|------|------|------|
| Original | 100M | - |
| gzip | 15M | 3s |
| bzip2 | 12M | 15s |
| xz | 10M | 45s |
| zstd | 14M | 1s |

Choose based on your needs: zstd for speed, xz for size, gzip for compatibility.

### Compressing in pipelines

```bash
# Compress output directly
mysqldump database | gzip > backup.sql.gz

# Decompress and process
zcat backup.sql.gz | mysql database

# Stream compressed over network
tar cf - /data | gzip | ssh remote "cat > backup.tar.gz"
```

</div>
</details>

---

<details>
<summary><strong>8. Archiving with tar</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

`tar` (tape archive) bundles multiple files into one archive. Combined with compression, it's the standard for backups and distribution.

### Basic tar operations

**Create archive:**

```bash
tar cvf archive.tar /path/to/files
```

| Flag | Meaning |
|------|---------|
| c | Create |
| v | Verbose |
| f | Filename follows |

**Extract archive:**

```bash
tar xvf archive.tar
tar xvf archive.tar -C /destination  # Extract to specific directory
```

| Flag | Meaning |
|------|---------|
| x | Extract |

**List contents:**

```bash
tar tvf archive.tar
```

### Compressed archives

Combine tar with compression:

```bash
# Create compressed archive
tar czvf archive.tar.gz /path/to/files   # gzip
tar cjvf archive.tar.bz2 /path/to/files  # bzip2
tar cJvf archive.tar.xz /path/to/files   # xz

# Extract compressed archive
tar xzvf archive.tar.gz
tar xjvf archive.tar.bz2
tar xJvf archive.tar.xz
```

Modern tar auto-detects compression:

```bash
tar xvf archive.tar.gz               # Works without z flag
tar xvf archive.tar.xz               # Works without J flag
```

### Common patterns

**Backup a directory:**

```bash
tar czvf backup-$(date +%Y%m%d).tar.gz /srv/chillspot-streaming/
```

**Exclude files:**

```bash
tar czvf backup.tar.gz --exclude="*.log" --exclude="node_modules" /app
```

**Extract single file:**

```bash
tar xzvf archive.tar.gz path/to/specific/file
```

**Preserve permissions (as root):**

```bash
sudo tar czvf backup.tar.gz --preserve-permissions /etc
sudo tar xzvf backup.tar.gz --preserve-permissions -C /
```

### Archive vs copy

For backups with many small files, tar is often faster than cp or rsync — one sequential write instead of many small operations:

```bash
# Slower: many file operations
cp -a /source /destination

# Faster: stream to tar, extract
tar cf - /source | tar xf - -C /destination
```

</div>
</details>

---

<details>
<summary><strong>9. Transferring Files</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Moving files between systems is daily DevOps work.

### scp — Secure copy

Simple file transfer over SSH:

```bash
# Local to remote
scp file.tar.gz user@remote:/path/

# Remote to local
scp user@remote:/path/file.tar.gz ./

# Recursive directory
scp -r /local/dir user@remote:/path/

# With specific port
scp -P 2222 file.tar.gz user@remote:/path/
```

### sftp — Interactive secure transfer

```bash
sftp user@remote
sftp> put localfile
sftp> get remotefile
sftp> ls
sftp> cd /path
sftp> bye
```

### rsync over SSH

More efficient than scp for large or repeated transfers:

```bash
rsync -avz /local/path/ user@remote:/remote/path/
rsync -avz user@remote:/remote/path/ /local/path/
```

Only transfers changed portions of files.

### wget and curl — Download from web

```bash
# Download file
wget https://example.com/file.tar.gz
curl -O https://example.com/file.tar.gz

# Download to specific name
wget -O output.tar.gz https://example.com/file.tar.gz
curl -o output.tar.gz https://example.com/file.tar.gz

# Resume interrupted download
wget -c https://example.com/largefile.iso
curl -C - -O https://example.com/largefile.iso
```

### Transferring large amounts

For massive transfers, combine tools:

```bash
# Compress and transfer in one pipeline
tar czf - /data | ssh user@remote "cat > /backup/data.tar.gz"

# Or extract on the remote side directly
tar czf - /data | ssh user@remote "tar xzf - -C /destination"
```

### Between cloud instances

Within cloud providers, use their tools:

```bash
# AWS S3
aws s3 cp file.tar.gz s3://bucket/
aws s3 sync /local/dir s3://bucket/path/

# GCP
gsutil cp file.tar.gz gs://bucket/
gsutil rsync -r /local/dir gs://bucket/path/

# Azure
az storage blob upload --file file.tar.gz --container-name mycontainer
```

These are often faster and cheaper for cloud-to-cloud transfers.

</div>
</details>

---

## 10. Where We Go From Here

You now manage storage end-to-end — from raw disks through partitions and filesystems to mounted volumes. You monitor usage, handle files at scale, compress efficiently, archive properly, and transfer data between systems.

But all these operations often need to be automated. Running backups manually doesn't scale. Repeating the same commands gets tedious and error-prone. Shell scripting transforms manual tasks into reliable, repeatable automation.

File 08 covers **Shell Scripting** — variables, conditionals, loops, functions, and the patterns that turn you from a command-line user into an automation engineer.

Your storage is managed. Let's automate everything else.
