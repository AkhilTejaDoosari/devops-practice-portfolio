# Logs, Monitoring & Troubleshooting

When things break — and they will break — logs tell you why. When systems slow down, monitoring shows you where. The difference between a junior admin who panics and a senior engineer who resolves issues calmly is systematic troubleshooting skill.

Linux systems generate enormous amounts of information. Every service writes logs. The kernel reports hardware events. systemd tracks service lifecycle. Applications record their activity. The challenge isn't getting information — it's knowing where to look and what to look for.

This file teaches you to read logs effectively, monitor system health, and troubleshoot methodically. By the end, you'll diagnose problems faster, understand what your systems are telling you, and build the observability that prevents problems before they escalate.

---

## Table of Contents

1. [Understanding Linux Logging](#1-understanding-linux-logging)
2. [Traditional Log Files](#2-traditional-log-files)
3. [systemd Journal](#3-systemd-journal)
4. [Log Management and Rotation](#4-log-management-and-rotation)
5. [System Monitoring](#5-system-monitoring)
6. [Resource Troubleshooting](#6-resource-troubleshooting)
7. [Service Troubleshooting](#7-service-troubleshooting)
8. [Network Troubleshooting](#8-network-troubleshooting)
9. [Building a Troubleshooting Mindset](#9-building-a-troubleshooting-mindset)
10. [Where We Go From Here](#10-where-we-go-from-here)

---

<details>
<summary><strong>1. Understanding Linux Logging</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Linux has two logging systems that coexist: traditional syslog-style text files and the modern systemd journal.

### Traditional logging (syslog)

The classic approach: services send messages to a syslog daemon (rsyslog, syslog-ng), which writes them to files in `/var/log/`. Text files, easily readable, easily searched with standard tools.

### systemd journal

Modern systems also use the systemd journal — a binary log that captures stdout/stderr from all services, kernel messages, and more. Accessed via `journalctl`, it offers structured queries and automatic metadata.

### Which to use?

Both. The journal captures everything systemd manages. Traditional logs remain for services that write directly to files, and many systems forward journal entries to syslog files for compatibility.

For ChillSpot:
- Service status and startup issues → `journalctl`
- Application logs → `/var/log/chillspot/` (if the app writes there)
- System events → both journal and `/var/log/syslog`

### Log levels

Messages have severity levels:

| Level | Meaning |
|-------|---------|
| emerg | System unusable |
| alert | Immediate action needed |
| crit | Critical conditions |
| err | Error conditions |
| warning | Warning conditions |
| notice | Normal but significant |
| info | Informational |
| debug | Debug messages |

Filter by level to focus on what matters. During an outage, start with errors and work down.

</div>
</details>

---

<details>
<summary><strong>2. Traditional Log Files</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Log files live in `/var/log/`. Know the important ones.

### Key log files

| File | Contains |
|------|----------|
| `/var/log/syslog` | General system messages (Debian/Ubuntu) |
| `/var/log/messages` | General system messages (RHEL/Rocky) |
| `/var/log/auth.log` | Authentication events (Debian/Ubuntu) |
| `/var/log/secure` | Authentication events (RHEL/Rocky) |
| `/var/log/kern.log` | Kernel messages |
| `/var/log/dmesg` | Boot-time kernel messages |
| `/var/log/apt/` | Package manager logs (Debian/Ubuntu) |
| `/var/log/dnf.log` | Package manager logs (RHEL) |
| `/var/log/nginx/` | Nginx access and error logs |
| `/var/log/mysql/` | MySQL logs |

### Reading logs

**View entire log:**

```bash
cat /var/log/syslog                  # Small files
less /var/log/syslog                 # Large files (scrollable)
```

**Follow in real-time:**

```bash
tail -f /var/log/syslog              # Watch new entries
tail -f /var/log/syslog | grep error # Filter while watching
```

**View last N lines:**

```bash
tail -100 /var/log/syslog
```

**Search for patterns:**

```bash
grep "error" /var/log/syslog
grep -i "failed" /var/log/auth.log   # Case insensitive
grep "Mar 15" /var/log/syslog        # Specific date
```

### Understanding log format

Typical syslog format:

```
Mar 15 10:30:45 hostname service[pid]: message
```

- Timestamp
- Hostname
- Service name and PID
- The actual message

### Combining log analysis

Find failed SSH logins:

```bash
grep "Failed password" /var/log/auth.log | tail -20
```

Count failures by IP:

```bash
grep "Failed password" /var/log/auth.log | \
    awk '{print $(NF-3)}' | sort | uniq -c | sort -rn | head
```

Find errors in the last hour:

```bash
grep "$(date -d '1 hour ago' '+%b %d %H')" /var/log/syslog | grep -i error
```

</div>
</details>

---

<details>
<summary><strong>3. systemd Journal</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

The journal captures everything from systemd-managed services with rich metadata.

### Basic journalctl usage

```bash
journalctl                           # All logs (oldest first)
journalctl -r                        # Reverse (newest first)
journalctl -f                        # Follow (like tail -f)
journalctl -n 100                    # Last 100 entries
```

### Filtering by unit

```bash
journalctl -u nginx                  # Nginx logs only
journalctl -u chillspot-streaming    # ChillSpot service
journalctl -u nginx -u postgresql    # Multiple services
```

### Filtering by time

```bash
journalctl --since "1 hour ago"
journalctl --since "2024-03-15 10:00" --until "2024-03-15 12:00"
journalctl --since today
journalctl --since yesterday --until today
journalctl -b                        # Current boot only
journalctl -b -1                     # Previous boot
```

### Filtering by priority

```bash
journalctl -p err                    # Errors and above
journalctl -p warning                # Warnings and above
journalctl -p err -u nginx           # Nginx errors only
```

### Output formats

```bash
journalctl -o short                  # Default, syslog-like
journalctl -o verbose                # All fields
journalctl -o json                   # JSON format
journalctl -o json-pretty            # Pretty JSON
journalctl -o cat                    # Just the message
```

### Searching

```bash
journalctl -g "error"                # Grep-style search
journalctl -u nginx -g "upstream"    # Search within service
```

### Kernel messages

```bash
journalctl -k                        # Kernel messages only
journalctl -k -b                     # Kernel messages this boot
dmesg                                # Also shows kernel ring buffer
dmesg -T                             # Human-readable timestamps
```

### Practical examples

**Why did a service fail to start?**

```bash
journalctl -u chillspot-streaming -b --no-pager
```

**What happened before the crash?**

```bash
journalctl -b -1 -p err              # Errors from previous boot
```

**Authentication issues?**

```bash
journalctl _COMM=sshd --since "1 hour ago"
```

### Journal size management

The journal can grow large. Check size:

```bash
journalctl --disk-usage
```

Limit size:

```bash
sudo journalctl --vacuum-size=500M   # Keep only 500MB
sudo journalctl --vacuum-time=7d     # Keep only 7 days
```

Configure permanently in `/etc/systemd/journald.conf`:

```ini
[Journal]
SystemMaxUse=500M
MaxRetentionSec=7day
```

Then restart: `sudo systemctl restart systemd-journald`

</div>
</details>

---

<details>
<summary><strong>4. Log Management and Rotation</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Logs grow indefinitely without management. Rotation archives old logs and prevents disk exhaustion.

### logrotate

Most distributions use logrotate to manage log files. Configuration lives in:

- `/etc/logrotate.conf` — Global settings
- `/etc/logrotate.d/` — Per-application configs

### Understanding logrotate config

```
/var/log/chillspot/*.log {
    daily                    # Rotate daily
    rotate 14                # Keep 14 rotated files
    compress                 # Compress rotated files
    delaycompress            # Compress on next rotation
    missingok                # Don't error if log missing
    notifempty               # Don't rotate empty files
    create 0640 chillspot chillspot   # Permissions for new file
    postrotate
        systemctl reload chillspot-streaming > /dev/null 2>&1 || true
    endscript
}
```

### Common rotation strategies

**Size-based:**

```
/var/log/app.log {
    size 100M
    rotate 5
}
```

**Time-based:**

```
/var/log/app.log {
    daily
    rotate 7
}
```

### Testing logrotate

```bash
sudo logrotate -d /etc/logrotate.d/chillspot    # Debug (dry run)
sudo logrotate -f /etc/logrotate.d/chillspot    # Force rotation
```

### Handling open files

Some applications hold log files open. After rotation, they keep writing to the old (now renamed) file. Solutions:

**copytruncate** — Copy and truncate the original:

```
/var/log/app.log {
    copytruncate
    daily
    rotate 7
}
```

**postrotate script** — Signal the application to reopen:

```
postrotate
    systemctl reload nginx > /dev/null 2>&1 || true
endscript
```

### Centralized logging

For multiple servers, forward logs to a central location:

- **rsyslog** — Can forward to remote syslog servers
- **Elasticsearch/Logstash/Kibana (ELK)** — Full-text search and visualization
- **Loki** — Lightweight log aggregation (pairs with Grafana)
- **Cloud services** — CloudWatch, Stackdriver, etc.

Basic rsyslog forwarding:

```
# /etc/rsyslog.d/50-forward.conf
*.* @logserver.chillspot.internal:514    # UDP
*.* @@logserver.chillspot.internal:514   # TCP
```

</div>
</details>

---

<details>
<summary><strong>5. System Monitoring</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Real-time monitoring shows current system state. Know these tools.

### top and htop

Live process view:

```bash
top
```

Key metrics at top:
- Load average (1, 5, 15 minutes)
- CPU breakdown (user, system, idle, wait)
- Memory usage
- Process list sorted by CPU

Inside top:
- `M` — Sort by memory
- `P` — Sort by CPU
- `k` — Kill process
- `1` — Show individual CPUs
- `q` — Quit

`htop` is friendlier with colors and mouse support.

### vmstat — Virtual memory stats

```bash
vmstat 1 5                           # Every 1 second, 5 times
```

Output:
```
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  0      0 512000  64000 256000    0    0    10    20  100  200 10  5 85  0  0
```

Key columns:
- `r` — Processes waiting for CPU
- `b` — Processes in uninterruptible sleep (usually I/O)
- `si/so` — Swap in/out (high = memory pressure)
- `wa` — CPU time waiting for I/O

### iostat — Disk I/O

```bash
iostat -xz 1                         # Extended stats, every 1 second
```

Watch for:
- `%util` — Disk utilization (100% = saturated)
- `await` — Average I/O wait time
- `r/s`, `w/s` — Reads/writes per second

### free — Memory usage

```bash
free -h
```

Output:
```
              total        used        free      shared  buff/cache   available
Mem:           16Gi        8Gi        2Gi       500Mi        6Gi         7Gi
Swap:          4Gi         0B         4Gi
```

**Available** is what matters — free plus reclaimable cache.

### uptime and load average

```bash
uptime
```

```
10:30:00 up 45 days, 3:22, 2 users, load average: 0.50, 0.75, 0.60
```

Load average: average number of processes wanting CPU over 1, 5, 15 minutes. On a 4-core system, load of 4.0 means fully utilized.

### sar — Historical data

If `sysstat` is installed and enabled:

```bash
sar -u                               # CPU history
sar -r                               # Memory history
sar -d                               # Disk history
sar -n DEV                           # Network history
sar -u -f /var/log/sa/sa15           # Specific day (15th)
```

### Quick health check script

```bash
#!/bin/bash
echo "=== System Health ==="
echo ""
echo "Uptime: $(uptime)"
echo ""
echo "=== CPU ==="
top -bn1 | head -5
echo ""
echo "=== Memory ==="
free -h
echo ""
echo "=== Disk ==="
df -h | grep -v tmpfs
echo ""
echo "=== Top Processes ==="
ps aux --sort=-%cpu | head -6
```

</div>
</details>

---

<details>
<summary><strong>6. Resource Troubleshooting</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

When systems slow down, identify which resource is constrained.

### High CPU

**Identify the culprit:**

```bash
top                                  # Sort by CPU (default)
ps aux --sort=-%cpu | head -10       # Top CPU consumers
```

**Is it user or system time?**

In `top`, look at `us` (user) vs `sy` (system). High system time suggests kernel activity — often I/O or context switching.

**What's the process doing?**

```bash
strace -p PID                        # System calls (careful, adds overhead)
perf top                             # CPU profiling (if perf installed)
```

### High memory

**What's using memory?**

```bash
ps aux --sort=-%mem | head -10       # Top memory consumers
```

**Is swap being used?**

```bash
free -h
swapon --show
vmstat 1                             # Watch si/so columns
```

High swap activity means memory pressure. Applications will slow dramatically.

**Clear cache (if needed):**

```bash
sync; echo 3 > /proc/sys/vm/drop_caches   # Drops page cache
```

This is rarely necessary — Linux manages cache well.

### High disk I/O

**What's causing I/O?**

```bash
iotop                                # Live I/O by process
iostat -xz 1                         # Disk stats
```

**Which files are being accessed?**

```bash
lsof +D /path/to/directory           # Open files in directory
fatrace                              # File access trace (if installed)
```

### High disk usage

**Find space consumers:**

```bash
df -h                                # Filesystem usage
du -sh /* 2>/dev/null | sort -rh | head   # Largest directories
find / -type f -size +100M 2>/dev/null    # Large files
```

**Common culprits:**
- `/var/log` — Logs not rotating
- `/tmp` — Temporary files accumulating
- `/var/lib/docker` — Docker images/containers

### Network saturation

**Check bandwidth:**

```bash
iftop                                # Live bandwidth by connection
nethogs                              # Bandwidth by process
nload                                # Simple bandwidth graph
```

**Connection counts:**

```bash
ss -s                                # Socket statistics summary
ss -tan | awk '{print $1}' | sort | uniq -c   # States
```

Many TIME_WAIT connections might indicate connection churn.

</div>
</details>

---

<details>
<summary><strong>7. Service Troubleshooting</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

When a service isn't working, follow a systematic approach.

### Check service status

```bash
systemctl status chillspot-streaming
```

Look for:
- Active state (running, failed, inactive)
- Recent log entries
- Main PID
- How long it's been running

### Read the logs

```bash
journalctl -u chillspot-streaming -n 50 --no-pager
journalctl -u chillspot-streaming -f   # Follow live
```

### Common failure patterns

**"Main process exited, code=exited, status=1"**

Application crashed. Check logs for the error message.

```bash
journalctl -u chillspot-streaming -b | tail -50
```

**"Address already in use"**

Port conflict. Find what's using it:

```bash
ss -tlnp | grep :8080
```

**"Permission denied"**

Service user can't access something. Check:

```bash
ls -la /path/to/file
namei -l /path/to/file               # Permissions along path
```

**Service keeps restarting:**

```bash
journalctl -u service --since "10 minutes ago" | grep -E "(start|stop|fail)"
```

Check if it's hitting resource limits:

```bash
systemctl show chillspot-streaming | grep -i limit
```

### Verify the service manually

Run what the service runs:

```bash
# Check the ExecStart from the unit file
systemctl cat chillspot-streaming

# Try running it manually as the service user
sudo -u chillspot /srv/chillspot-streaming/app --config /path/to/config
```

Manual execution often gives better error messages.

### Check dependencies

```bash
systemctl list-dependencies chillspot-streaming
systemctl status postgresql          # If it depends on database
```

### Reload vs restart

```bash
systemctl reload nginx               # Reload config, keep connections
systemctl restart nginx              # Full restart, drops connections
```

Use reload when possible — it's less disruptive.

</div>
</details>

---

<details>
<summary><strong>8. Network Troubleshooting</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Network issues require systematic layer-by-layer diagnosis.

### The troubleshooting ladder

Work from bottom to top:

**1. Is the interface up?**

```bash
ip link show
ip addr show
```

**2. Do you have an IP?**

```bash
ip addr show eth0
```

**3. Can you reach the gateway?**

```bash
ip route show                        # Find gateway
ping -c 3 192.168.1.1                # Ping it
```

**4. Can you reach external IPs?**

```bash
ping -c 3 8.8.8.8
```

**5. Does DNS work?**

```bash
dig google.com
nslookup google.com
```

**6. Can you reach the destination?**

```bash
ping destination.com
traceroute destination.com
```

**7. Is the port open?**

```bash
nc -zv destination.com 443
curl -v https://destination.com
```

### Common issues

**"Network unreachable"**

No route to destination. Check routing:

```bash
ip route show
ip route get 8.8.8.8
```

**"Connection refused"**

Port is closed or service not listening:

```bash
ss -tlnp | grep :PORT                # Is anything listening?
```

**"Connection timed out"**

Firewall blocking, or host down:

```bash
# Check local firewall
sudo iptables -L -n
sudo ufw status

# Trace the path
traceroute destination.com
mtr destination.com
```

**DNS failures:**

```bash
cat /etc/resolv.conf                 # Check nameservers
dig @8.8.8.8 domain.com              # Try different DNS
systemd-resolve --status             # If using systemd-resolved
```

### Capturing traffic

When you need to see exactly what's happening:

```bash
sudo tcpdump -i eth0 port 80         # HTTP traffic
sudo tcpdump -i eth0 host 192.168.1.50   # Traffic to/from host
sudo tcpdump -i eth0 -w capture.pcap # Save for Wireshark
```

### Testing from the application's perspective

```bash
curl -v http://localhost:8080/health
curl -v --connect-timeout 5 http://external-api.com/endpoint

# Test with specific source IP
curl --interface eth0 http://api.com

# Test SSL/TLS
openssl s_client -connect host:443
```

</div>
</details>

---

<details>
<summary><strong>9. Building a Troubleshooting Mindset</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Effective troubleshooting is methodical, not random.

### The scientific method

1. **Observe** — What exactly is the symptom?
2. **Hypothesize** — What could cause this?
3. **Test** — Check your hypothesis
4. **Conclude** — Was it the cause?
5. **Iterate** — If not, next hypothesis

Don't change things randomly hoping something works.

### Gather information first

Before fixing, understand:

```bash
# What changed recently?
last                                 # Recent logins
rpm -qa --last | head -20            # Recent package changes (RHEL)
zcat /var/log/apt/history.log.*.gz | tail -50   # Recent apt changes

# When did it start?
journalctl --since "2 hours ago" -p err

# What's different from working systems?
diff /etc/nginx/nginx.conf /etc/nginx/nginx.conf.bak
```

### Isolate the problem

Narrow down systematically:

- Does it affect all users or just one?
- All servers or just one?
- All endpoints or just one?
- Started suddenly or gradual degradation?

### Check the obvious first

80% of issues are simple:

- Is the service running?
- Is there disk space?
- Did someone change something?
- Is the network reachable?
- Are credentials correct?

### Document as you go

Note what you checked and what you found:

```
10:30 - Users report slow API
10:32 - Checked service status - running
10:33 - CPU at 95%, postgres process
10:35 - Found slow query in pg_stat_activity
10:40 - Killed query, CPU normalized
10:45 - Root cause: missing index on users table
```

This helps if you need to escalate, and builds knowledge for next time.

### Know when to escalate

If you've spent 30 minutes without progress:

- Document what you've tried
- Gather relevant logs
- Escalate with clear information

Fresh eyes often spot what you missed.

### Post-incident

After resolving:

1. Document root cause
2. Document resolution
3. Identify preventive measures
4. Implement monitoring to catch it earlier

ChillSpot's API was slow → Missing index → Added index → Added query time monitoring.

### Build your runbook

Document common issues and their resolutions:

```markdown
## ChillSpot API Slow

### Symptoms
- Response times > 2s
- User complaints

### Checks
1. `systemctl status chillspot-streaming`
2. `top` - check CPU
3. `free -h` - check memory
4. `journalctl -u chillspot-streaming -n 100`
5. Check database: `psql -c "SELECT * FROM pg_stat_activity"`

### Common Causes
- Database slow query → Kill query, add index
- Memory pressure → Check for leaks, restart if needed
- Upstream API timeout → Check external dependencies
```

</div>
</details>

---

## 10. Where We Go From Here

You now read logs effectively, monitor system health, and troubleshoot methodically. You know where to look when things break, how to isolate problems, and how to document what you find.

All these skills — filesystems, permissions, processes, networking, storage, scripting, monitoring — come together when you deploy real applications. Production servers, cloud instances, containers, and orchestration systems all build on these fundamentals.

File 10 covers **Deploying on Servers & Cloud** — bringing everything together into production deployments, cloud infrastructure, containers, and the operational practices that keep systems running reliably.

You understand Linux deeply. Let's put it to work.
