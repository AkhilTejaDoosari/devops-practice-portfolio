# **10. Security, Validation & Best Practices**

You've written Terraform code. It works. Resources spin up, state is tracked, modules are reusable.

But here's the reality: working code isn't enough.

One day, someone commits an AWS access key to GitHub. Your repository is public for 3 minutes before you notice. In those 3 minutes, a bot finds it, spins up 50 EC2 instances for crypto mining, and you wake up to a $12,000 AWS bill.

Or someone deploys to production without validation. A typo in a CIDR block cuts off database access. The app goes down. Customers are locked out.

Or your Terraform code works, but it's a mess. Files are named inconsistently. Variables are scattered. No one knows what anything does. Six months later, you can't even understand your own code.

**Security isn't optional. Validation isn't extra. Organization isn't cosmetic.**

They're the difference between infrastructure that survives production and infrastructure that creates disasters.

This file covers how to protect secrets, validate inputs, organize code, test configurations, optimize performance, and troubleshoot when things break.

---

## Table of Contents
1. [Security Fundamentals (Never Commit Secrets)](#1-security-fundamentals-never-commit-secrets)
2. [Sensitive Data Management](#2-sensitive-data-management)
3. [Input Validation Rules](#3-input-validation-rules)
4. [Code Organization & Naming Conventions](#4-code-organization--naming-conventions)
5. [Testing Strategies](#5-testing-strategies)
6. [Performance & Optimization](#6-performance--optimization)
7. [Troubleshooting & Debugging](#7-troubleshooting--debugging)

---

<details>
<summary><strong>1. Security Fundamentals (Never Commit Secrets)</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

The #1 rule in Terraform security:

**Never. Commit. Secrets. To. Git.**

Not in `.tf` files. Not in `.tfvars` files. Not in comments. Not "just for testing." Not "just this once." Never.

**Why this matters:**

Git history is permanent. Even if you delete a file, the secret is still in commit history. Even if you force-push to overwrite history, someone might have already cloned the repo. Even if your repo is private, one misconfiguration makes it public.

Bots scan GitHub continuously. When they find AWS keys, they use them within minutes.

**What NOT to do:**

```hcl
# ❌ NEVER DO THIS
provider "aws" {
  access_key = "AKIAIOSFODNN7EXAMPLE"
  secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
}

# ❌ NEVER DO THIS
resource "aws_db_instance" "main" {
  username = "admin"
  password = "MySecretPassword123"
}

# ❌ NEVER DO THIS (even in comments)
# Old password: MyOldPassword456
```

**What happens when secrets leak:**

1. Bots detect the secret within minutes
2. Attackers use the credentials immediately
3. Resources are created in your account (crypto miners, data exfiltration)
4. You get massive AWS bills
5. You scramble to rotate credentials and assess damage
6. Your security team is very, very unhappy

**The correct approach:**

**For cloud provider credentials:**
Use AWS CLI profiles, environment variables, or IAM roles. Never hardcode.

```hcl
# ✅ Use AWS CLI profile
provider "aws" {
  profile = "my-profile"
  region  = "us-east-1"
}

# ✅ Or use environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
provider "aws" {
  region = "us-east-1"
}

# ✅ Or use IAM roles (best for EC2, ECS, Lambda)
provider "aws" {
  region = "us-east-1"
  # No credentials needed - instance profile provides them
}
```

**For passwords and secrets in resources:**
Reference them from secret management systems. Never hardcode.

```hcl
# ✅ Reference from AWS Secrets Manager
data "aws_secretsmanager_secret_version" "db_password" {
  secret_id = "prod/db/password"
}

resource "aws_db_instance" "main" {
  username = "admin"
  password = data.aws_secretsmanager_secret_version.db_password.secret_string
}
```

**Protect your .tfvars files:**

If you use `.tfvars` files for sensitive values, add them to `.gitignore`:

```gitignore
# .gitignore
*.tfvars
*.tfvars.json
.terraform/
terraform.tfstate
terraform.tfstate.backup
```

**What to do if you accidentally commit a secret:**

1. **Immediately rotate the credentials** (create new ones, revoke old ones)
2. **Remove the secret from Git history** (use `git filter-branch` or BFG Repo-Cleaner)
3. **Force push to overwrite history** (warn your team first)
4. **Assume the secret is compromised** (even if you act fast, bots are faster)
5. **Check CloudTrail for unauthorized activity**

**The golden rule:**

If you're about to type a password, API key, or access token into a Terraform file, stop. Find another way. Use environment variables, secret managers, or IAM roles. Your future self will thank you.

</div>

</details>

---

<details>
<summary><strong>2. Sensitive Data Management</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

You need secrets in infrastructure. Databases need passwords. APIs need keys. Applications need tokens.

The question isn't whether to use secrets. It's how to manage them safely.

**Three tools for managing secrets in Terraform:**

**1. AWS Secrets Manager** (AWS-native, automatic rotation)
**2. HashiCorp Vault** (Multi-cloud, enterprise features)
**3. AWS Systems Manager Parameter Store** (Simple, cheap, AWS-native)

**AWS Secrets Manager**

Best for: Database credentials, API keys, anything that needs automatic rotation.

**Store a secret (AWS CLI):**
```bash
aws secretsmanager create-secret \
  --name prod/db/password \
  --secret-string "MySecurePassword123"
```

**Retrieve in Terraform:**
```hcl
data "aws_secretsmanager_secret_version" "db_password" {
  secret_id = "prod/db/password"
}

resource "aws_db_instance" "main" {
  engine               = "postgres"
  instance_class       = "db.t3.micro"
  username             = "admin"
  password             = data.aws_secretsmanager_secret_version.db_password.secret_string
  
  lifecycle {
    ignore_changes = [password]
  }
}
```

**Why `ignore_changes = [password]`?**
If Secrets Manager rotates the password, Terraform won't try to "fix" it back to the old value.

**Automatic rotation:**
```hcl
resource "aws_secretsmanager_secret_rotation" "db_password" {
  secret_id           = aws_secretsmanager_secret.db_password.id
  rotation_lambda_arn = aws_lambda_function.rotate_secret.arn

  rotation_rules {
    automatically_after_days = 30
  }
}
```

**AWS Systems Manager Parameter Store**

Best for: Configuration values, non-critical secrets, when cost matters.

**Store a parameter:**
```hcl
resource "aws_ssm_parameter" "db_password" {
  name  = "/prod/db/password"
  type  = "SecureString"  # Encrypted with KMS
  value = var.db_password  # Never hardcode this
}
```

**Retrieve in Terraform:**
```hcl
data "aws_ssm_parameter" "db_password" {
  name = "/prod/db/password"
}

resource "aws_db_instance" "main" {
  username = "admin"
  password = data.aws_ssm_parameter.db_password.value
}
```

**Parameter Store vs Secrets Manager:**

| Feature | Parameter Store | Secrets Manager |
|---------|----------------|-----------------|
| Cost | Free (standard), $0.05/param (advanced) | $0.40/secret/month + API calls |
| Rotation | Manual | Automatic |
| Versioning | Yes | Yes |
| Cross-region replication | Advanced tier only | Built-in |
| Best for | Config values, simple secrets | Database credentials, API keys |

**HashiCorp Vault**

Best for: Multi-cloud environments, dynamic secrets, enterprise security requirements.

**Configure Vault provider:**
```hcl
provider "vault" {
  address = "https://vault.example.com"
  token   = var.vault_token  # Provided via environment variable
}

data "vault_generic_secret" "db_password" {
  path = "secret/prod/db"
}

resource "aws_db_instance" "main" {
  username = "admin"
  password = data.vault_generic_secret.db_password.data["password"]
}
```

**Dynamic secrets (Vault generates credentials on-demand):**
```hcl
data "vault_aws_access_credentials" "aws" {
  backend = "aws"
  role    = "deploy"
}

provider "aws" {
  access_key = data.vault_aws_access_credentials.aws.access_key
  secret_key = data.vault_aws_access_credentials.aws.secret_key
}
```

Vault creates temporary AWS credentials that expire after a set time. No long-lived keys to rotate.

**Marking outputs as sensitive:**

If an output contains secret data, mark it:

```hcl
output "db_password" {
  value     = aws_db_instance.main.password
  sensitive = true
}
```

This prevents Terraform from showing the value in CLI output. It still appears in state, so protect your state file (covered in File 08).

**Environment variables for secrets:**

For secrets that Terraform itself needs (not resources), use environment variables:

```bash
export TF_VAR_db_password="MySecurePassword"
```

Terraform automatically reads `TF_VAR_*` environment variables as input variables.

**Key principle:**

Secrets should live in secret management systems, not in Terraform code. Terraform retrieves them at runtime, uses them to create resources, but never stores them in version control.

</div>

</details>

---

<details>
<summary><strong>3. Input Validation Rules</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Someone passes `instance_type = "t99.mega"` to your module. Terraform accepts it. `terraform apply` runs. AWS rejects it. The deployment fails 5 minutes in.

Or someone sets `cidr_block = "10.0.0.0/8"` instead of `10.0.0.0/16`. Your VPC is now 16 million IP addresses instead of 65,000. Good luck with that IP management.

**Validation catches mistakes before they become disasters.**

Terraform provides three validation mechanisms:
1. **Variable validation** (custom rules)
2. **Preconditions** (check before creation)
3. **Postconditions** (verify after creation)

**Variable Validation**

Define validation rules directly in variable blocks:

```hcl
variable "instance_type" {
  type        = string
  description = "EC2 instance type"

  validation {
    condition     = can(regex("^t[23]\\.(nano|micro|small|medium|large)$", var.instance_type))
    error_message = "Instance type must be a valid t2 or t3 type (nano, micro, small, medium, large)."
  }
}

variable "environment" {
  type        = string
  description = "Environment name"

  validation {
    condition     = contains(["dev", "staging", "prod"], var.environment)
    error_message = "Environment must be dev, staging, or prod."
  }
}

variable "cidr_block" {
  type        = string
  description = "VPC CIDR block"

  validation {
    condition     = can(cidrhost(var.cidr_block, 0))
    error_message = "CIDR block must be valid IPv4 CIDR notation."
  }

  validation {
    condition     = tonumber(split("/", var.cidr_block)[1]) >= 16
    error_message = "CIDR block must be /16 or smaller (e.g., 10.0.0.0/16)."
  }
}
```

**Common validation patterns:**

**Enforce string length:**
```hcl
validation {
  condition     = length(var.name) >= 3 && length(var.name) <= 63
  error_message = "Name must be between 3 and 63 characters."
}
```

**Require specific prefixes:**
```hcl
validation {
  condition     = can(regex("^prod-", var.resource_name))
  error_message = "Production resources must start with 'prod-'."
}
```

**Validate number ranges:**
```hcl
variable "instance_count" {
  type = number

  validation {
    condition     = var.instance_count >= 1 && var.instance_count <= 10
    error_message = "Instance count must be between 1 and 10."
  }
}
```

**Validate against lists:**
```hcl
variable "region" {
  type = string

  validation {
    condition     = contains(["us-east-1", "us-west-2", "eu-west-1"], var.region)
    error_message = "Region must be us-east-1, us-west-2, or eu-west-1."
  }
}
```

**Preconditions (Lifecycle Checks)**

Validate resource configuration before creation:

```hcl
resource "aws_instance" "web" {
  ami           = var.ami_id
  instance_type = var.instance_type

  lifecycle {
    precondition {
      condition     = data.aws_ami.selected.architecture == "x86_64"
      error_message = "AMI must be x86_64 architecture."
    }

    precondition {
      condition     = data.aws_subnet.selected.availability_zone != "us-east-1a"
      error_message = "Cannot deploy to us-east-1a (reserved for legacy apps)."
    }
  }
}
```

**Postconditions (Verify After Creation)**

Confirm resource state after creation:

```hcl
resource "aws_db_instance" "main" {
  allocated_storage    = 20
  engine              = "postgres"
  instance_class      = "db.t3.micro"
  multi_az            = var.enable_multi_az

  lifecycle {
    postcondition {
      condition     = self.multi_az == true
      error_message = "Database must have Multi-AZ enabled."
    }

    postcondition {
      condition     = self.storage_encrypted == true
      error_message = "Database storage must be encrypted."
    }
  }
}
```

**When to use each:**

| Validation Type | When to Use | Example |
|----------------|-------------|---------|
| **Variable validation** | Check user inputs | Valid instance types, CIDR ranges |
| **Precondition** | Check external data before creation | Verify AMI exists, check subnet availability |
| **Postcondition** | Verify resource state after creation | Confirm encryption enabled, Multi-AZ active |

**Validation best practices:**

1. **Fail fast** — Validate inputs before Terraform contacts the cloud provider
2. **Clear error messages** — Tell users what's wrong and how to fix it
3. **Validate early** — Catch issues in `terraform plan`, not `terraform apply`
4. **Document constraints** — Use validation as self-documenting requirements

**Example: Comprehensive validation:**

```hcl
variable "instance_config" {
  type = object({
    type         = string
    ami          = string
    environment  = string
  })

  validation {
    condition     = contains(["t2.micro", "t2.small", "t3.micro", "t3.small"], var.instance_config.type)
    error_message = "Instance type must be t2/t3 micro or small."
  }

  validation {
    condition     = can(regex("^ami-[a-f0-9]{17}$", var.instance_config.ami))
    error_message = "AMI must be valid format (ami-xxxxxxxxxxxxxxxxx)."
  }

  validation {
    condition     = contains(["dev", "staging", "prod"], var.instance_config.environment)
    error_message = "Environment must be dev, staging, or prod."
  }
}
```

**When validation runs:**
- `terraform validate` — Checks syntax only, doesn't run validation rules
- `terraform plan` — Runs all validation rules
- `terraform apply` — Runs all validation rules again before applying

Validation catches mistakes before they cost money or cause downtime.

</div>

</details>

---

<details>
<summary><strong>4. Code Organization & Naming Conventions</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Six months from now, you'll open your Terraform project and wonder what past-you was thinking.

**Why is this file called `stuff.tf`? What's in `main2.tf`? Why are there three `variables.tf` files?**

Good code organization isn't about being neat. It's about making your code understandable to future-you and everyone else who has to work with it.

**Standard File Structure**

Every Terraform project should have these files:

```
project/
├── main.tf           # Primary resources
├── variables.tf      # Input variable definitions
├── outputs.tf        # Output value definitions
├── versions.tf       # Provider and Terraform version constraints
├── terraform.tfvars  # Variable values (DO NOT COMMIT if sensitive)
└── README.md         # Documentation
```

**What goes where:**

**`main.tf`** — Core resources for your infrastructure
```hcl
resource "aws_vpc" "main" {
  cidr_block = var.vpc_cidr
}

resource "aws_subnet" "public" {
  vpc_id     = aws_vpc.main.id
  cidr_block = var.public_subnet_cidr
}
```

**`variables.tf`** — Input variable declarations (no values)
```hcl
variable "vpc_cidr" {
  type        = string
  description = "CIDR block for VPC"
}

variable "environment" {
  type        = string
  description = "Environment name (dev, staging, prod)"
}
```

**`outputs.tf`** — Values to expose after apply
```hcl
output "vpc_id" {
  value       = aws_vpc.main.id
  description = "ID of the created VPC"
}
```

**`versions.tf`** — Provider version constraints
```hcl
terraform {
  required_version = ">= 1.5.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}
```

**`terraform.tfvars`** — Variable values (add to .gitignore if sensitive)
```hcl
vpc_cidr    = "10.0.0.0/16"
environment = "prod"
```

**When projects grow:**

For larger projects, split resources into logical files:

```
project/
├── main.tf              # Project overview, data sources
├── vpc.tf               # VPC, subnets, route tables
├── compute.tf           # EC2, Auto Scaling
├── database.tf          # RDS, DynamoDB
├── security.tf          # Security groups, NACLs
├── variables.tf         # All variables
├── outputs.tf           # All outputs
├── versions.tf          # Provider versions
└── terraform.tfvars     # Variable values
```

**Multi-environment structure:**

For managing multiple environments (dev, staging, prod):

```
project/
├── modules/
│   └── vpc/
│       ├── main.tf
│       ├── variables.tf
│       └── outputs.tf
├── environments/
│   ├── dev/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   ├── terraform.tfvars
│   │   └── backend.tf
│   ├── staging/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   ├── terraform.tfvars
│   │   └── backend.tf
│   └── prod/
│       ├── main.tf
│       ├── variables.tf
│       ├── terraform.tfvars
│       └── backend.tf
```

Each environment has its own state, backend, and variable values.

**Naming Conventions**

Consistent naming prevents confusion and makes code searchable.

**Resource naming pattern:**
```
<resource_type>_<purpose>_<environment>
```

**Examples:**
```hcl
resource "aws_vpc" "main" { ... }                    # Simple, single VPC
resource "aws_subnet" "public_a" { ... }             # Public subnet in AZ a
resource "aws_subnet" "private_b" { ... }            # Private subnet in AZ b
resource "aws_security_group" "web" { ... }          # Security group for web servers
resource "aws_instance" "app_server" { ... }         # Application server instance
```

**Variable naming:**
```hcl
variable "vpc_cidr" { ... }              # VPC CIDR block
variable "public_subnet_cidrs" { ... }   # List of public subnet CIDRs
variable "instance_type" { ... }         # EC2 instance type
variable "enable_monitoring" { ... }     # Boolean flag
variable "tags" { ... }                  # Common tags
```

**Output naming:**
```hcl
output "vpc_id" { ... }               # VPC ID
output "public_subnet_ids" { ... }    # List of public subnet IDs
output "db_endpoint" { ... }          # Database connection endpoint
```

**Module naming:**
```
modules/
├── vpc/           # Creates VPC with subnets
├── compute/       # EC2, Auto Scaling
├── database/      # RDS setup
└── monitoring/    # CloudWatch, alarms
```

**Tagging strategy:**

Every resource should have consistent tags:

```hcl
locals {
  common_tags = {
    Environment = var.environment
    Project     = var.project_name
    ManagedBy   = "Terraform"
    Owner       = var.owner
    CostCenter  = var.cost_center
  }
}

resource "aws_instance" "web" {
  ami           = var.ami_id
  instance_type = var.instance_type

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-web-${var.environment}"
      Role = "webserver"
    }
  )
}
```

**Documentation in code:**

```hcl
# Creates a VPC with public and private subnets across 2 availability zones.
# Public subnets have internet gateway access.
# Private subnets use NAT gateway for outbound traffic.
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = merge(
    local.common_tags,
    {
      Name = "${var.project_name}-vpc-${var.environment}"
    }
  )
}
```

**README.md template:**

Every project needs a README:

```markdown
# Project Name

## Overview
Brief description of what this infrastructure creates.

## Prerequisites
- Terraform >= 1.5.0
- AWS CLI configured
- Required AWS permissions

## Usage
```bash
terraform init
terraform plan
terraform apply
```

## Inputs
| Name | Description | Type | Default |
|------|-------------|------|---------|
| vpc_cidr | VPC CIDR block | string | - |
| environment | Environment name | string | - |

## Outputs
| Name | Description |
|------|-------------|
| vpc_id | ID of the created VPC |
| subnet_ids | List of subnet IDs |
```

**Code organization best practices:**

1. **One resource type per file** for large projects
2. **Group related resources** in medium projects
3. **Keep files under 300 lines** — split if larger
4. **Use meaningful names** — `web_server` not `resource1`
5. **Document complex logic** — Future-you will appreciate it
6. **Consistent formatting** — Use `terraform fmt`

Clean code organization makes maintenance easier, onboarding faster, and debugging less painful.

</div>

</details>

---

<details>
<summary><strong>5. Testing Strategies</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

You wrote Terraform code. It looks right. But does it work? Is it secure? Does it follow best practices?

Testing infrastructure code prevents:
- Syntax errors that fail during apply
- Security misconfigurations (open security groups, unencrypted storage)
- Cost overruns (accidentally creating expensive resources)
- Compliance violations (resources in wrong regions, missing tags)

**Five testing tools:**

1. **`terraform fmt`** — Code formatting
2. **`terraform validate`** — Syntax validation
3. **`tflint`** — Linting and best practices
4. **`checkov`** — Security and compliance scanning
5. **`terraform plan`** — Execution plan review

**1. terraform fmt (Format Code)**

Formats code to HashiCorp's standard style. Fixes indentation, spacing, alignment.

```bash
terraform fmt
```

**Before:**
```hcl
resource "aws_instance" "web"{
ami="ami-123456"
  instance_type= "t2.micro"
    tags={
  Name="web-server"
    }
}
```

**After:**
```hcl
resource "aws_instance" "web" {
  ami           = "ami-123456"
  instance_type = "t2.micro"
  tags = {
    Name = "web-server"
  }
}
```

**Check formatting without changing files:**
```bash
terraform fmt -check
```

Returns exit code 0 if formatted correctly, non-zero if changes needed. Perfect for CI/CD pipelines.

**Format recursively:**
```bash
terraform fmt -recursive
```

**2. terraform validate (Syntax Validation)**

Checks configuration syntax and internal consistency.

```bash
terraform init
terraform validate
```

**What it checks:**
- Valid HCL syntax
- Resource block structure
- Required arguments present
- Variable references exist
- Output references valid

**Example errors caught:**

```hcl
# Missing required argument
resource "aws_instance" "web" {
  instance_type = "t2.micro"
  # Error: Missing required argument "ami"
}

# Invalid reference
resource "aws_subnet" "public" {
  vpc_id     = aws_vpc.nonexistent.id  # Error: No aws_vpc.nonexistent
  cidr_block = "10.0.1.0/24"
}
```

**What it doesn't check:**
- Whether AMI IDs exist
- Whether CIDR blocks are valid
- Security best practices
- AWS API errors

**3. TFLint (Terraform Linter)**

Catches errors `terraform validate` misses. Checks for deprecated syntax, invalid values, best practices.

**Install:**
```bash
# macOS
brew install tflint

# Linux
curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash

# Windows
choco install tflint
```

**Run:**
```bash
tflint
```

**What it finds:**

**Invalid instance types:**
```hcl
resource "aws_instance" "web" {
  ami           = "ami-123456"
  instance_type = "t99.mega"  # TFLint: Invalid instance type
}
```

**Deprecated syntax:**
```hcl
resource "aws_db_instance" "main" {
  name = "mydb"  # TFLint: 'name' is deprecated, use 'db_name'
}
```

**Missing required providers:**
```hcl
# TFLint: Missing provider version constraint
resource "aws_instance" "web" {
  ami = "ami-123456"
}
```

**Configure TFLint (.tflint.hcl):**
```hcl
plugin "aws" {
  enabled = true
  version = "0.21.0"
  source  = "github.com/terraform-linters/tflint-ruleset-aws"
}

rule "aws_instance_invalid_type" {
  enabled = true
}

rule "terraform_deprecated_syntax" {
  enabled = true
}
```

**4. Checkov (Security & Compliance Scanner)**

Scans for security issues, compliance violations, and best practices.

**Install:**
```bash
pip install checkov
```

**Run:**
```bash
checkov -d .
```

**What it finds:**

**Unencrypted storage:**
```hcl
resource "aws_s3_bucket" "data" {
  bucket = "my-data-bucket"
  # Checkov: S3 bucket encryption not enabled
}
```

**Public security groups:**
```hcl
resource "aws_security_group" "web" {
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]  # Checkov: SSH open to world
  }
}
```

**Missing Multi-AZ:**
```hcl
resource "aws_db_instance" "main" {
  engine         = "postgres"
  instance_class = "db.t3.micro"
  multi_az       = false  # Checkov: RDS not Multi-AZ
}
```

**Output:**
```
Check: CKV_AWS_19: "Ensure the S3 bucket has server-side encryption enabled"
	FAILED for resource: aws_s3_bucket.data
	File: /s3.tf:1-4

Check: CKV_AWS_23: "Ensure the security group doesn't allow SSH from 0.0.0.0/0"
	FAILED for resource: aws_security_group.web
	File: /security.tf:5-12
```

**Skip specific checks:**
```hcl
resource "aws_security_group" "web" {
  # checkov:skip=CKV_AWS_23:SSH needed for emergency access
  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
```

**5. terraform plan (Execution Plan Review)**

The final test before applying changes.

```bash
terraform plan -out=tfplan
```

**What to check:**

**Resource changes:**
```
# aws_instance.web will be created
+ resource "aws_instance" "web" {
    + ami           = "ami-123456"
    + instance_type = "t2.micro"
  }
```

**Resource replacements (destructive):**
```
# aws_db_instance.main must be replaced
-/+ resource "aws_db_instance" "main" {
      ~ instance_class = "db.t3.micro" -> "db.t3.small"
    }
```

**Sensitive value warnings:**
```
# aws_db_instance.main will be created
+ resource "aws_db_instance" "main" {
    + password = (sensitive value)
  }
```

**Review plan output before applying:**
- Count of resources to add/change/destroy
- Any unexpected replacements
- Sensitive values properly hidden
- Resource dependencies correct

**Save and review plan file:**
```bash
terraform plan -out=tfplan
terraform show tfplan
```

**Testing workflow:**

```bash
# 1. Format code
terraform fmt -recursive

# 2. Validate syntax
terraform validate

# 3. Lint code
tflint

# 4. Security scan
checkov -d .

# 5. Generate plan
terraform plan -out=tfplan

# 6. Review plan
terraform show tfplan

# 7. Apply if all checks pass
terraform apply tfplan
```

**Automate testing in CI/CD:**

```yaml
# .github/workflows/terraform.yml
name: Terraform Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
      
      - name: Format check
        run: terraform fmt -check -recursive
      
      - name: Init
        run: terraform init
      
      - name: Validate
        run: terraform validate
      
      - name: TFLint
        run: |
          curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash
          tflint
      
      - name: Checkov
        run: |
          pip install checkov
          checkov -d .
      
      - name: Plan
        run: terraform plan
```

Testing catches issues before they reach production. Format, validate, lint, scan, plan — then apply.

</div>

</details>

---

<details>
<summary><strong>6. Performance & Optimization</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Your Terraform code works. But `terraform plan` takes 5 minutes. `terraform apply` takes 20. Making a small change requires re-checking hundreds of resources.

**Performance matters when:**
- You have hundreds of resources
- You're managing multiple environments
- You're running Terraform in CI/CD
- Your team is waiting for deployments

**Common performance bottlenecks:**

1. **Too many resources in one state** → Split into multiple states
2. **Slow provider API calls** → Use `-refresh=false` when appropriate
3. **Large state files** → Use `-target` for specific changes
4. **Excessive data source queries** → Cache results in locals
5. **Deep module nesting** → Flatten module structure

**1. Split Large State Files**

**Problem:** One state file with 500 resources. Every `plan` checks all 500.

**Solution:** Split into logical boundaries.

**Before:**
```
project/
└── main.tf  # All 500 resources
```

**After:**
```
project/
├── networking/   # VPC, subnets (separate state)
├── compute/      # EC2, ASG (separate state)
├── database/     # RDS (separate state)
└── monitoring/   # CloudWatch (separate state)
```

Each has its own state. Changes to compute don't refresh networking.

**2. Use -refresh=false**

By default, `terraform plan` refreshes state from real infrastructure. For large deployments, this is slow.

```bash
# Normal plan (slow, refreshes everything)
terraform plan

# Fast plan (skips refresh)
terraform plan -refresh=false
```

**When to use `-refresh=false`:**
- Code-only changes (variable updates, syntax fixes)
- You know infrastructure hasn't changed outside Terraform
- You're just checking syntax

**When NOT to use it:**
- Someone might have modified resources manually
- You're checking for drift
- Before applying changes to production

**3. Target Specific Resources**

When changing one resource, don't plan the entire infrastructure.

```bash
# Plan only specific resources
terraform plan -target=aws_instance.web

# Plan a module
terraform plan -target=module.vpc

# Plan multiple targets
terraform plan -target=aws_instance.web -target=aws_security_group.web
```

**Warning:** `-target` can miss dependencies. Use carefully.

**4. Optimize Data Source Queries**

**Problem:** Querying the same data multiple times.

```hcl
resource "aws_instance" "web1" {
  ami    = data.aws_ami.ubuntu.id  # API call
  subnet_id = data.aws_subnet.public.id  # API call
}

resource "aws_instance" "web2" {
  ami    = data.aws_ami.ubuntu.id  # API call again
  subnet_id = data.aws_subnet.public.id  # API call again
}
```

**Solution:** Cache in locals.

```hcl
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"]
  
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

locals {
  ubuntu_ami = data.aws_ami.ubuntu.id
}

resource "aws_instance" "web1" {
  ami = local.ubuntu_ami  # No API call
}

resource "aws_instance" "web2" {
  ami = local.ubuntu_ami  # No API call
}
```

**5. Parallelize When Possible**

Terraform runs operations in parallel when dependencies allow.

**Bad (sequential):**
```hcl
resource "aws_instance" "web1" {
  ami = var.ami_id
}

resource "aws_instance" "web2" {
  ami           = var.ami_id
  depends_on    = [aws_instance.web1]  # Unnecessary dependency
}
```

**Good (parallel):**
```hcl
resource "aws_instance" "web1" {
  ami = var.ami_id
}

resource "aws_instance" "web2" {
  ami = var.ami_id
  # No dependency, both create in parallel
}
```

**Control parallelism:**
```bash
# Default: 10 parallel operations
terraform apply

# Increase parallelism (use carefully)
terraform apply -parallelism=20

# Decrease parallelism (safer for API rate limits)
terraform apply -parallelism=5
```

**6. Use State Locking**

Remote state with locking prevents concurrent runs. This isn't just safety — it's performance.

Multiple people running `terraform apply` simultaneously creates conflicts, failures, and wasted time.

```hcl
terraform {
  backend "s3" {
    bucket         = "my-terraform-state"
    key            = "prod/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-locks"  # Locking
  }
}
```

If someone else is running Terraform, you wait. Better than corrupting state.

**7. Minimize Provider Configurations**

Multiple provider aliases create separate sessions.

```hcl
provider "aws" {
  region = "us-east-1"
}

provider "aws" {
  alias  = "west"
  region = "us-west-2"
}

provider "aws" {
  alias  = "eu"
  region = "eu-west-1"
}
```

Each provider maintains its own API connections. More providers = more overhead.

**If possible, use one provider and pass region as variable.**

**8. Optimize Remote State Reads**

Every module that uses `terraform_remote_state` makes an S3 API call.

```hcl
data "terraform_remote_state" "vpc" {
  backend = "s3"
  config = {
    bucket = "my-terraform-state"
    key    = "vpc/terraform.tfstate"
    region = "us-east-1"
  }
}
```

If 5 modules read the same remote state, that's 5 S3 reads. Cache outputs in locals if used multiple times.

**Performance checklist:**

✅ Split large state files into logical components
✅ Use `-refresh=false` for code-only changes
✅ Target specific resources when possible
✅ Cache data source results in locals
✅ Remove unnecessary `depends_on`
✅ Use appropriate parallelism settings
✅ Enable state locking (prevents conflicts)
✅ Minimize provider configurations
✅ Cache remote state reads

**When to optimize:**
- `terraform plan` takes > 2 minutes
- `terraform apply` takes > 10 minutes
- You're managing > 100 resources
- Your team complains about slowness

Start with the biggest bottleneck first. Usually it's splitting state or using `-refresh=false`.

</div>

</details>

---

<details>
<summary><strong>7. Troubleshooting & Debugging</strong></summary>

<div style="margin-left: 16px; margin-right: 16px; margin-top: 8px; margin-bottom: 8px;">

Something broke. `terraform apply` failed. Resources are stuck. State is corrupted. The error message makes no sense.

**Welcome to debugging Terraform.**

Every issue falls into one of these categories:
1. **Syntax errors** (code is wrong)
2. **Provider errors** (AWS rejected the request)
3. **State issues** (state doesn't match reality)
4. **Dependency problems** (resources created in wrong order)
5. **Permission issues** (IAM doesn't allow the action)

**Enable Debug Logging**

Terraform's error messages are sometimes cryptic. Enable detailed logs:

```bash
export TF_LOG=DEBUG
terraform apply
```

**Log levels:**
- `TRACE` — Most verbose, shows every API call
- `DEBUG` — Detailed debugging info
- `INFO` — General informational messages
- `WARN` — Warning messages
- `ERROR` — Error messages only

**Save logs to a file:**
```bash
export TF_LOG=DEBUG
export TF_LOG_PATH=./terraform-debug.log
terraform apply
```

**Turn off logging:**
```bash
unset TF_LOG
unset TF_LOG_PATH
```

**Common Error Patterns**

**1. Resource Already Exists**

**Error:**
```
Error: Error creating EC2 instance: InvalidParameterValue: 
Instance with name 'web-server' already exists
```

**Cause:** Resource exists in AWS but not in Terraform state.

**Solution:** Import the resource.
```bash
terraform import aws_instance.web i-0abc123def456
terraform plan  # Verify it matches
```

**2. Dependency Cycle**

**Error:**
```
Error: Cycle: aws_instance.web, aws_security_group.web
```

**Cause:** Resource A depends on B, B depends on A.

```hcl
resource "aws_instance" "web" {
  security_groups = [aws_security_group.web.id]
}

resource "aws_security_group" "web" {
  name = aws_instance.web.id  # Circular dependency
}
```

**Solution:** Remove the circular reference. Security groups don't need instance IDs.

**3. Invalid Resource Attribute**

**Error:**
```
Error: Unsupported attribute: 'public_ip_address' 
is not a valid attribute for aws_instance
```

**Cause:** Typo or wrong attribute name.

**Solution:** Check provider documentation. Correct attribute:
```hcl
# Wrong
output "ip" {
  value = aws_instance.web.public_ip_address
}

# Right
output "ip" {
  value = aws_instance.web.public_ip
}
```

**4. State Lock Timeout**

**Error:**
```
Error: Error acquiring the state lock
Lock Info:
  ID:        abc123-def456
  Operation: OperationTypeApply
  Who:       user@hostname
  Created:   2024-01-15 10:30:00 UTC
```

**Cause:** Someone else is running Terraform, or a previous run crashed.

**Solution:**

**If someone is actually running Terraform:** Wait for them to finish.

**If the lock is stale (crashed run):**
```bash
terraform force-unlock abc123-def456
```

**Warning:** Only use `force-unlock` if you're absolutely sure no one else is running Terraform.

**5. AWS Permission Denied**

**Error:**
```
Error: Error creating EC2 instance: UnauthorizedOperation: 
You are not authorized to perform this operation
```

**Cause:** IAM user/role lacks required permissions.

**Solution:** Check IAM policies. Required actions:
```json
{
  "Effect": "Allow",
  "Action": [
    "ec2:RunInstances",
    "ec2:DescribeInstances",
    "ec2:TerminateInstances"
  ],
  "Resource": "*"
}
```

**Debug IAM issues:**
```bash
aws sts get-caller-identity  # Check who you are
aws iam get-user  # Check your user
```

**6. Resource Stuck in State**

**Error:**
```
Error: resource 'aws_instance.web' still exists in state 
but was deleted in AWS
```

**Cause:** Someone deleted the resource manually in AWS console.

**Solution:** Remove from state.
```bash
terraform state rm aws_instance.web
terraform plan  # Verify it's gone
```

**7. State Drift Detected**

**Error:**
```
Note: Objects have changed outside of Terraform
Terraform detected the following changes made outside of Terraform 
since the last "terraform apply"
```

**Cause:** Someone modified resources directly in AWS.

**Solution:**

**Option 1:** Accept the changes (update state to match reality)
```bash
terraform apply -refresh-only
```

**Option 2:** Revert changes (force Terraform's version)
```bash
terraform apply -replace=aws_instance.web
```

**8. Provider Plugin Crash**

**Error:**
```
Error: Plugin did not respond
The plugin encountered an error and failed to respond to the plugin.(*GRPCProvider).ApplyResourceChange call
```

**Cause:** Provider bug or corrupted cache.

**Solution:**
```bash
rm -rf .terraform
terraform init
terraform apply
```

**9. Timeout Errors**

**Error:**
```
Error: timeout while waiting for resource to be created
```

**Cause:** Resource creation took too long (default timeout exceeded).

**Solution:** Increase timeout.
```hcl
resource "aws_db_instance" "main" {
  allocated_storage = 20
  engine           = "postgres"

  timeouts {
    create = "60m"  # Default is 40m
    delete = "60m"
  }
}
```

**10. Sensitive Value in Output**

**Error:**
```
Error: Output refers to sensitive values
Cannot output sensitive value 'db_password'
```

**Solution:** Mark output as sensitive.
```hcl
output "db_password" {
  value     = aws_db_instance.main.password
  sensitive = true
}
```

**Debugging Workflow**

```bash
# 1. Enable debug logging
export TF_LOG=DEBUG
export TF_LOG_PATH=./debug.log

# 2. Run the failing command
terraform apply

# 3. Review the error message
# Look for the first "Error:" line

# 4. Check the debug log
cat debug.log | grep -i error

# 5. Verify state consistency
terraform state list
terraform state show aws_instance.web

# 6. Refresh state from AWS
terraform apply -refresh-only

# 7. Check AWS console
# Verify resource actually exists/doesn't exist

# 8. Try targeted operations
terraform plan -target=aws_instance.web

# 9. Clean and reinit if needed
rm -rf .terraform .terraform.lock.hcl
terraform init

# 10. Turn off debug logging
unset TF_LOG
unset TF_LOG_PATH
```

**Troubleshooting Commands**

| Command | Purpose |
|---------|---------|
| `terraform state list` | List all resources in state |
| `terraform state show <resource>` | Show resource details |
| `terraform state rm <resource>` | Remove resource from state |
| `terraform import <resource> <id>` | Add existing resource to state |
| `terraform force-unlock <id>` | Release stuck state lock |
| `terraform apply -refresh-only` | Update state without changes |
| `terraform apply -replace=<resource>` | Force resource recreation |
| `terraform plan -out=tfplan` | Save plan for inspection |
| `terraform show tfplan` | Display saved plan |

**State Recovery**

If state is corrupted:

```bash
# 1. Backup current state
cp terraform.tfstate terraform.tfstate.backup

# 2. List state versions (if using S3 backend)
aws s3api list-object-versions --bucket my-terraform-state --prefix terraform.tfstate

# 3. Download previous version
aws s3api get-object --bucket my-terraform-state --key terraform.tfstate --version-id <version-id> terraform.tfstate.recovered

# 4. Test recovered state
terraform state list -state=terraform.tfstate.recovered

# 5. If good, replace current state
mv terraform.tfstate.recovered terraform.tfstate
```

**Prevention is better than debugging:**

✅ Always run `terraform plan` before `apply`
✅ Use version control (Git) for all Terraform code
✅ Enable state locking (prevents concurrent runs)
✅ Use remote state with versioning (S3 + versioning enabled)
✅ Never edit state files manually
✅ Test in dev before deploying to prod
✅ Document infrastructure changes
✅ Use consistent naming conventions

When something breaks, stay calm. Check logs. Verify state. Consult documentation. Most issues are fixable without destroying everything.

</div>

</details>

---

**You've learned how to build infrastructure safely.**

Security protects secrets. Validation catches mistakes. Organization keeps code maintainable. Testing prevents disasters. Optimization speeds up deployments. Troubleshooting fixes issues when they arise.

These aren't extra steps. They're the foundation of production-ready infrastructure.

Next, we'll bring everything together in **real-world projects** — complete architectures with VPCs, Auto Scaling, load balancers, databases, CI/CD, and monitoring.